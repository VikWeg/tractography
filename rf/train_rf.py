import os
import argparse

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from dipy.io.gradients import read_bvals_bvecs
import pickle
import multiprocessing

import yaml

from hashlib import md5

os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)


def train_model(config):
    sample_path = os.path.join(config["train_dir"], "samples.npz")

    hasher = md5()
    for v in config.values():
        hasher.update(str(v).encode())

    out_dir = os.path.join(config['out_dir'], config["model_name"], hasher.hexdigest())
    if os.path.exists(out_dir):
        print("This model config has been trained already:\n{}".format(out_dir))
        return

    samples = np.load(sample_path, "samples.npz")
    inputs = samples["inputs"]
    outputs = samples["outputs"]
    inputs = inputs[:min(config["max_n_samples"], len(inputs))]
    outputs = outputs[:min(config["max_n_samples"], len(outputs))]

    _, bvecs = read_bvals_bvecs(None, config["bvecs"])

    output_classes = np.array([np.argmax([np.dot(base_vec, outvec) for base_vec in bvecs]) for outvec in outputs])

    clf = RandomForestClassifier(n_estimators=config["n_estimators"],
                                 max_depth=config["max_depth"],
                                 verbose=1,
                                 n_jobs=multiprocessing.cpu_count(),
                                 random_state=0)
    clf.fit(inputs, output_classes)

    os.makedirs(out_dir, exist_ok=True)
    config_path = os.path.join(out_dir, "config" + ".yml")
    print("Saving {}".format(config_path))
    with open(config_path, "w") as file:
        yaml.dump(config, file, default_flow_style=False)

    model_path = os.path.join(out_dir, 'model')
    print("Saving {}".format(model_path))
    with open(model_path, 'wb') as f:
        pickle.dump(clf, f)

    return clf, inputs, outputs, output_classes


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="Train the entrack model")

    parser.add_argument("train_dir", type=str,
        help="Path to the dir where training samples generated by `generate_conditional_samples.py` are saved")

    parser.add_argument("--bvecs", type=str, required=True,
                        help="Path to bvec file. These vectors are used as classes for the classification.")

    parser.add_argument("--eval_dir", type=str, default=None,
        help="Path to the dir where evaluation samples generated by `generate_conditional_samples.py` are saved")

    parser.add_argument("--n_estimators", type=int, default=1000,
                        help="Number of estimators of random forest")

    parser.add_argument("--max_depth", type=int, default=25,
                        help="Maximum depth of random forest")

    parser.add_argument("--max_n_samples", type=int, default=np.inf,
        help="Maximum number of samples to be used for both training and evaluation")

    parser.add_argument("--out_dir", type=str, default='../models/',
        help="Directory to save the training results")

    args = parser.parse_args()

    config = dict(
        model_name="random_forest",
        train_dir=args.train_dir,
        eval_dir=args.eval_dir,
        bvecs=args.bvecs,
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        max_n_samples=args.max_n_samples,
        out_dir=args.out_dir
    )

    train_model(config)