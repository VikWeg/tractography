{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import yaml\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import nipy as ni\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from GPUtil import getFirstAvailable\n",
    "from tensorflow.keras import backend as K\n",
    "from hashlib import md5\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use only one trk/dwi pair for now, TODO: average over both instances\n",
    "OR: implement matching fibers over the two instances\n",
    "predicted fibers should be resampled only at predicted points (so that we don't smooth it out too much)\n",
    "TODO: calculate separate prior agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement(config):\n",
    "    \n",
    "    hasher = md5()\n",
    "    for v in config.values():\n",
    "        hasher.update(str(v).encode())\n",
    "    \n",
    "    subject_dir = os.path.join(\"subjects\", config[\"subject\"])\n",
    "    \n",
    "    save_dir = os.path.join(subject_dir, \"agreement\", hasher.hexdigest())\n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"Agreement with this config has been calculated already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    sample_path_1 = os.path.join(subject_dir, \"samples\", config[\"sample_dirs\"][0], \"samples.npz\")\n",
    "    samples_1 = np.load(sample_path_1)\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    model_path_1 = os.path.join(\"models\", config[\"model_dirs\"][0], \"model.h5\")\n",
    "    model_1 = load_model(\n",
    "        model_path_1,\n",
    "        custom_objects={\"negative_log_likelihood\": negative_log_likelihood,\n",
    "                        \"DistributionLambda\": tfp.layers.DistributionLambda})\n",
    "    \n",
    "    model_path_2 = os.path.join(\"models\", config[\"model_dirs\"][1], \"model.h5\")\n",
    "    model_2 = load_model(\n",
    "        model_path_2,\n",
    "        custom_objects={\"negative_log_likelihood\": negative_log_likelihood,\n",
    "                        \"DistributionLambda\": tfp.layers.DistributionLambda})\n",
    "    \n",
    "    def c(kappa):\n",
    "        return kappa / (4 * np.pi * np.sinh(kappa))\n",
    "    \n",
    "    batch_size = 2**15 # 32768\n",
    "    n_samples = len(samples_1[\"inputs\"])\n",
    "    n_batches = np.ceil(n_samples / batch_size).astype(int)\n",
    "    \n",
    "    local_agreement = np.array([])\n",
    "    for i in range(3):#n_batches):\n",
    "    \n",
    "        posterior_1 = model_1(samples_1[\"inputs\"][i*batch_size:(i+1)*batch_size])\n",
    "        posterior_2 = model_2(samples_1[\"inputs\"][i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        mu_1 = posterior_1.mean()\n",
    "        mu_2 = posterior_2.mean()\n",
    "\n",
    "        kappa_1 = posterior_1.concentration.numpy()\n",
    "        kappa_2 = posterior_2.concentration.numpy()\n",
    "\n",
    "        kappa_12 = np.linalg.norm(mu_1 + mu_2, axis=1)\n",
    "\n",
    "        agreement = c(kappa_1) * c(kappa_2) / c(kappa_12)\n",
    "        \n",
    "        local_agreement = np.hstack([local_agreement, agreement])\n",
    "        \n",
    "        print(\"Finished {:4d}/{} batches.\".format(i+1, n_batches), end=\"\\r\")\n",
    "    \n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "    config_path = os.path.join(save_dir, \"config.yml\")\n",
    "    print(\"Saving {}\".format(config_path))\n",
    "    config[\"agreement_mean\"] = str(local_agreement.mean())\n",
    "    config[\"agreement_std\"] = str(local_agreement.std())\n",
    "    with open(config_path, \"w\") as file:\n",
    "        yaml.dump(config, file, default_flow_style=False) \n",
    "    \n",
    "    return local_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    subject = \"992774\",\n",
    "    sample_dirs = [\"fa7c02604b92de5f32cd3b61dbc2f8b7\", \"samples_2\"],\n",
    "    model_dirs = [\"entrack_conditional/8d5593b08d4548286cc8564373e82e11\",\n",
    "                  \"entrack_conditional/8d5593b08d4548286cc8564373e82e11\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving subjects/992774/agreement/a0f910610ec7b183514e22fad9bace7c/config.yml\n"
     ]
    }
   ],
   "source": [
    "local_agreement = agreement(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
