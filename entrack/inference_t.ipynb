{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "import nipy as ni\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from GPUtil import getFirstAvailable\n",
    "from tensorflow.keras import backend as K\n",
    "from nibabel.streamlines.tck import TckFile\n",
    "from nibabel.streamlines.array_sequence import ArraySequence, concatenate\n",
    "from nibabel.streamlines.tractogram import Tractogram\n",
    "from hashlib import md5\n",
    "from sklearn.preprocessing import normalize\n",
    "from pathos.multiprocessing import Pool\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from time import time\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config):\n",
    "    \"\"\"\n",
    "    seeds: [[x1,y1,z1],...,[xn,yn,zn]]\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    \n",
    "    subject_dir = os.path.join(\"..\", \"subjects\", config[\"subject\"])\n",
    "    \n",
    "    hasher = md5()\n",
    "    for v in config.values():\n",
    "        hasher.update(str(v).encode())\n",
    "    \n",
    "    save_dir = os.path.join(subject_dir, \"predicted_fibers\", hasher.hexdigest())\n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"Predictions with this config have been created already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    print(\"Loading DWI...\")\n",
    "    \n",
    "    dwi_path = os.path.join(subject_dir, config[\"dwi_path\"])\n",
    "    dwi_img = ni.load_image(dwi_path)\n",
    "    dwi = dwi_img.get_data()\n",
    "    \n",
    "    print(\"Loading Model...\")\n",
    "    \n",
    "    model_path = os.path.join(config[\"model_dir\"], \"model.h5\")\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    model = load_model(model_path,\n",
    "                       custom_objects={\"negative_log_likelihood\": negative_log_likelihood,\n",
    "                                       \"DistributionLambda\": tfp.layers.DistributionLambda})\n",
    "    \n",
    "    input_shape = model.layers[0].get_output_at(0).get_shape().as_list()[-1]\n",
    "    block_size = int(np.cbrt(input_shape / dwi.shape[-1]))\n",
    "    \n",
    "    def ijk2xyz(indices):\n",
    "        indices = np.hstack([indices, np.zeros((len(indices),1))])\n",
    "        return dwi_img.coordmap(indices)[:, :3]\n",
    "    \n",
    "    def xyz2ijk(coords, snap=False):\n",
    "        coords = np.hstack([coords, np.zeros((len(coords),1))])\n",
    "        ijk = dwi_img.coordmap.inverse()(coords)[:, :3]\n",
    "        if snap:\n",
    "            ijk = np.round(ijk).astype(int)\n",
    "        return np.clip(ijk,\n",
    "                       np.tile([0,0,0], (len(ijk), 1)) + (block_size // 2),\n",
    "                       np.tile(dwi.shape[:3], (len(ijk), 1)) - (block_size // 2) - 1\n",
    "        )\n",
    "    \n",
    "    # Define Fiber Termination\n",
    "    \n",
    "    class Terminator(object):\n",
    "        def __init__(self):\n",
    "            if \".nii\" in config[\"terminator\"][0]:\n",
    "                scalar_img = ni.load_image(os.path.join(subject_dir, config[\"terminator\"][0]))\n",
    "                self.scalar = scalar_img.get_data()\n",
    "            else:\n",
    "                raise NotImplementedError # TODO: Implement termination model\n",
    "            self.threshold = config[\"terminator\"][1]\n",
    "        def __call__(self, ijk):\n",
    "            if hasattr(self, \"scalar\"):\n",
    "                ijk = np.round(ijk).astype(int)\n",
    "                return np.where(self.scalar[ijk[:,0],ijk[:,1],ijk[:,2]] < self.threshold)[0]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    terminator = Terminator()\n",
    "    \n",
    "    print(\"Loading Seeds...\")\n",
    "    \n",
    "    seeds = np.load(os.path.join(subject_dir, config[\"seeds\"]))\n",
    "    # Duplicate seeds for positive and negative starting direction\n",
    "    seeds = np.vstack([seeds, seeds])\n",
    "    \n",
    "    # Define Prior for First Fiber Direction\n",
    "    \n",
    "    class Prior(object):\n",
    "        def __init__(self):\n",
    "            if \".nii\" in config[\"prior\"]:\n",
    "                peak_img = ni.load_image(os.path.join(subject_dir, config[\"prior\"]))\n",
    "                self.peak = peak_img.get_data()\n",
    "            elif \".h5\" in config:\n",
    "                raise NotImplementedError # TODO: Implement prior model\n",
    "                \n",
    "        def __call__(self, ijk):\n",
    "            if hasattr(self, \"peak\"):\n",
    "                ijk = np.round(ijk).astype(int)\n",
    "                # Assuming that seeds have been duplicated\n",
    "                return self.peak[ijk[:,0],ijk[:,1],ijk[:,2]] * np.repeat([[1],[-1]], len(ijk)/2, axis=0)\n",
    "            elif hasattr(self, \"model\"):\n",
    "                raise NotImplementedError # TODO: Implement prior model\n",
    "        \n",
    "    prior = Prior()\n",
    "    \n",
    "    # Define Interpolation\n",
    "    \n",
    "    def interpolate(ijk):\n",
    "\n",
    "        def inpol_fn(idx):\n",
    "            IDX = np.round(idx).astype(int)\n",
    "\n",
    "            values = np.zeros([3, 3, 3,\n",
    "                               block_size, block_size, block_size,\n",
    "                               dwi.shape[-1]])\n",
    "\n",
    "            for x in range(block_size):\n",
    "                for y in range(block_size):\n",
    "                    for z in range(block_size):\n",
    "                        values[x, y, z,:] = dwi[\n",
    "                            IDX[0] + x - 2 * (block_size // 2) : IDX[0] + x + 1,\n",
    "                            IDX[1] + y - 2 * (block_size // 2) : IDX[1] + y + 1,\n",
    "                            IDX[2] + z - 2 * (block_size // 2) : IDX[2] + z + 1,\n",
    "                            :]\n",
    "                        \n",
    "            fn = RegularGridInterpolator(([-1,0,1],[-1,0,1],[-1,0,1]), values)\n",
    "\n",
    "            d = fn([idx[0]-IDX[0], idx[1]-IDX[1], idx[2]-IDX[2]])[0]\n",
    "            \n",
    "            return d.flatten()\n",
    "        \n",
    "        with Pool(processes=20) as pool:  \n",
    "            return pool.map(inpol_fn, ijk)\n",
    "    \n",
    "    print(\"Initialize Fibers...\")\n",
    "    \n",
    "    ijk = xyz2ijk(seeds)\n",
    "    xyz = seeds.reshape(-1, 1, 3) # [fiber, pt, coo]\n",
    "    fiber_idx = np.hstack([np.arange(len(seeds)//2), np.arange(len(seeds)//2)])\n",
    "    fibers = [[] for _ in range(len(seeds)//2)]\n",
    "    \n",
    "    print(\"Start Iteration...\")\n",
    "    \n",
    "    for i in range(config[\"max_steps\"]):\n",
    "        t0 = time()\n",
    "        \n",
    "        if config[\"interpolate_dwi\"]:\n",
    "            d = interpolate(ijk)\n",
    "        else:\n",
    "            ijk = np.round(ijk).astype(int)\n",
    "\n",
    "            d = np.zeros([len(ijk), block_size, block_size, block_size, dwi.shape[-1]])\n",
    "            \n",
    "            for ii, idx in enumerate(ijk):\n",
    "                d[ii] = dwi[idx[0] - (block_size // 2) : idx[0] + (block_size // 2) + 1,\n",
    "                            idx[1] - (block_size // 2) : idx[1] + (block_size // 2) + 1,\n",
    "                            idx[2] - (block_size // 2) : idx[2] + (block_size // 2) + 1,\n",
    "                        :]\n",
    "                \n",
    "            d = d.reshape(-1, dwi.shape[-1] * block_size**3)\n",
    "        \n",
    "        if i == 0:\n",
    "            vin = prior(xyz[:,0,:])\n",
    "        else:\n",
    "            vin = vout.copy()\n",
    "        \n",
    "        if config[\"predict_fn\"] == \"mean\":\n",
    "            vout = model(np.hstack([vin,d])).mean().numpy()\n",
    "            vout = normalize(vout) # Careful, the FvM mean is not a unit vector!\n",
    "        else:\n",
    "            vout = model(np.hstack([vin,d])).sample().numpy() # Samples are unit length, though!\n",
    "        \n",
    "        rout = (xyz[:,-1,:] + config[\"step_size\"] * vout).reshape(-1, 1, 3)\n",
    "        \n",
    "        xyz = np.concatenate([xyz, rout], axis=1)\n",
    "        \n",
    "        ijk = xyz2ijk(xyz[:,-1,:])\n",
    "        \n",
    "        terminal_indices = terminator(ijk)\n",
    "\n",
    "        for idx in terminal_indices:\n",
    "            gidx = fiber_idx[idx]\n",
    "            # Other end not yet added\n",
    "            if not fibers[gidx]:\n",
    "                fibers[gidx].append(xyz[idx])\n",
    "            # Other end already added\n",
    "            else:\n",
    "                this_end = xyz[idx]\n",
    "                other_end = fibers[gidx][0]\n",
    "                merged_fiber = np.vstack([np.flip(this_end[1:], axis=0), other_end]) # stitch ends together\n",
    "                fibers[gidx] = [merged_fiber]\n",
    "                \n",
    "        xyz = np.delete(xyz, terminal_indices, axis=0)\n",
    "        vout = np.delete(vout, terminal_indices, axis=0)\n",
    "        fiber_idx = np.delete(fiber_idx, terminal_indices)\n",
    "        \n",
    "        ijk = xyz2ijk(xyz[:,-1,:])\n",
    "        \n",
    "        print(\"Iter {:4d}/{}, finished {:5d}/{:5d} ({:3.0f}%) of all seeds with {:6.0f} steps/sec\".format(\n",
    "            (i+1), config[\"max_steps\"], len(seeds)-len(fiber_idx), len(seeds),\n",
    "            100*(1-len(fiber_idx)/len(seeds)), len(vin) / (time() - t0), end=\"\\r\")\n",
    "        )\n",
    "        if len(fiber_idx) == 0:\n",
    "            break\n",
    "    \n",
    "    # Include unfinished fibers:\n",
    "    \n",
    "    for idx, gidx in enumerate(fiber_idx):\n",
    "        if not fibers[gidx]:\n",
    "            fibers[gidx].append(xyz[idx])\n",
    "        else:\n",
    "            this_end = xyz[idx]\n",
    "            other_end = fibers[gidx][0]\n",
    "            merged_fiber = np.vstack([np.flip(this_end[1:], axis=0), other_end])\n",
    "            fibers[gidx] = [merged_fiber]\n",
    "    \n",
    "    # Save Result\n",
    "    \n",
    "    fibers = [f[0] for f in fibers]\n",
    "    \n",
    "    tractogram = Tractogram(\n",
    "        streamlines=ArraySequence(fibers),\n",
    "        affine_to_rasmm=np.eye(4)\n",
    "    )\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    fiber_path = os.path.join(save_dir, \"fibers.trk\")\n",
    "    print(\"\\nSaving {}\".format(fiber_path))\n",
    "    nib.streamlines.save(tractogram, fiber_path)\n",
    "\n",
    "    config_path = os.path.join(save_dir, \"config.yml\")\n",
    "    print(\"Saving {}\".format(config_path))\n",
    "    with open(config_path, \"w\") as file:\n",
    "        yaml.dump(config, file, default_flow_style=False) \n",
    "    \n",
    "    return tractogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    subject=\"992774\",\n",
    "    model_dir=\"../models/entrack_conditional/8d5593b08d4548286cc8564373e82e11\",\n",
    "    dwi_path=\"fod.nii.gz\",\n",
    "    prior=\"peak.nii.gz\",\n",
    "    seeds=\"seeds/bcc2e734e49c597e25ece8a3d499d060/seeds.npy\",\n",
    "    terminator=[\"fa.nii.gz\", 0.21],\n",
    "    predict_fn=\"mean\",\n",
    "    interpolate_dwi=False,\n",
    "    step_size=0.2,\n",
    "    max_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DWI...\n",
      "Loading Model...\n",
      "Loading Seeds...\n",
      "Initialize Fibers...\n",
      "Start Iteration...\n",
      "Iter    1/100, finished 138917/364160 ( 38%) of all seeds with 105672 steps/sec\n",
      "Iter    2/100, finished 149175/364160 ( 41%) of all seeds with 120591 steps/sec\n",
      "Iter    3/100, finished 159289/364160 ( 44%) of all seeds with  96373 steps/sec\n",
      "Iter    4/100, finished 168847/364160 ( 46%) of all seeds with 132386 steps/sec\n",
      "Iter    5/100, finished 178168/364160 ( 49%) of all seeds with 128756 steps/sec\n",
      "Iter    6/100, finished 186622/364160 ( 51%) of all seeds with 112993 steps/sec\n",
      "Iter    7/100, finished 194479/364160 ( 53%) of all seeds with 126785 steps/sec\n",
      "Iter    8/100, finished 201545/364160 ( 55%) of all seeds with 125309 steps/sec\n",
      "Iter    9/100, finished 207896/364160 ( 57%) of all seeds with 121051 steps/sec\n",
      "Iter   10/100, finished 213426/364160 ( 59%) of all seeds with  91243 steps/sec\n",
      "Iter   11/100, finished 218077/364160 ( 60%) of all seeds with 124993 steps/sec\n",
      "Iter   12/100, finished 222566/364160 ( 61%) of all seeds with 127154 steps/sec\n",
      "Iter   13/100, finished 226788/364160 ( 62%) of all seeds with 122311 steps/sec\n",
      "Iter   14/100, finished 230682/364160 ( 63%) of all seeds with 121000 steps/sec\n",
      "Iter   15/100, finished 234289/364160 ( 64%) of all seeds with 119331 steps/sec\n",
      "Iter   16/100, finished 237702/364160 ( 65%) of all seeds with 114210 steps/sec\n",
      "Iter   17/100, finished 240457/364160 ( 66%) of all seeds with 119383 steps/sec\n",
      "Iter   18/100, finished 243176/364160 ( 67%) of all seeds with 117892 steps/sec\n",
      "Iter   19/100, finished 245433/364160 ( 67%) of all seeds with 126520 steps/sec\n",
      "Iter   20/100, finished 247605/364160 ( 68%) of all seeds with 130242 steps/sec\n",
      "Iter   21/100, finished 249690/364160 ( 69%) of all seeds with 125153 steps/sec\n",
      "Iter   22/100, finished 251566/364160 ( 69%) of all seeds with 114451 steps/sec\n",
      "Iter   23/100, finished 253307/364160 ( 70%) of all seeds with 119195 steps/sec\n",
      "Iter   24/100, finished 254936/364160 ( 70%) of all seeds with 109731 steps/sec\n",
      "Iter   25/100, finished 256357/364160 ( 70%) of all seeds with 114333 steps/sec\n",
      "Iter   26/100, finished 257616/364160 ( 71%) of all seeds with 115806 steps/sec\n",
      "Iter   27/100, finished 258761/364160 ( 71%) of all seeds with 116519 steps/sec\n",
      "Iter   28/100, finished 259939/364160 ( 71%) of all seeds with 111337 steps/sec\n",
      "Iter   29/100, finished 260975/364160 ( 72%) of all seeds with 104773 steps/sec\n",
      "Iter   30/100, finished 261905/364160 ( 72%) of all seeds with 108851 steps/sec\n",
      "Iter   31/100, finished 262869/364160 ( 72%) of all seeds with 104341 steps/sec\n",
      "Iter   32/100, finished 263835/364160 ( 72%) of all seeds with 101890 steps/sec\n",
      "Iter   33/100, finished 264707/364160 ( 73%) of all seeds with 109503 steps/sec\n",
      "Iter   34/100, finished 265583/364160 ( 73%) of all seeds with 106004 steps/sec\n",
      "Iter   35/100, finished 266424/364160 ( 73%) of all seeds with 111935 steps/sec\n",
      "Iter   36/100, finished 267277/364160 ( 73%) of all seeds with 105238 steps/sec\n",
      "Iter   37/100, finished 267994/364160 ( 74%) of all seeds with 113981 steps/sec\n",
      "Iter   38/100, finished 268878/364160 ( 74%) of all seeds with 118778 steps/sec\n",
      "Iter   39/100, finished 269627/364160 ( 74%) of all seeds with 117590 steps/sec\n",
      "Iter   40/100, finished 270315/364160 ( 74%) of all seeds with 113779 steps/sec\n",
      "Iter   41/100, finished 271125/364160 ( 74%) of all seeds with 115714 steps/sec\n",
      "Iter   42/100, finished 271791/364160 ( 75%) of all seeds with 108189 steps/sec\n",
      "Iter   43/100, finished 272482/364160 ( 75%) of all seeds with 111572 steps/sec\n",
      "Iter   44/100, finished 273110/364160 ( 75%) of all seeds with 108761 steps/sec\n",
      "Iter   45/100, finished 273650/364160 ( 75%) of all seeds with  99634 steps/sec\n",
      "Iter   46/100, finished 274287/364160 ( 75%) of all seeds with 104903 steps/sec\n",
      "Iter   47/100, finished 274892/364160 ( 75%) of all seeds with 110235 steps/sec\n",
      "Iter   48/100, finished 275468/364160 ( 76%) of all seeds with 108063 steps/sec\n",
      "Iter   49/100, finished 276002/364160 ( 76%) of all seeds with 103724 steps/sec\n",
      "Iter   50/100, finished 276530/364160 ( 76%) of all seeds with 106689 steps/sec\n",
      "Iter   51/100, finished 277020/364160 ( 76%) of all seeds with 108247 steps/sec\n",
      "Iter   52/100, finished 277591/364160 ( 76%) of all seeds with 103520 steps/sec\n",
      "Iter   53/100, finished 278140/364160 ( 76%) of all seeds with 107222 steps/sec\n",
      "Iter   54/100, finished 278687/364160 ( 77%) of all seeds with 107697 steps/sec\n",
      "Iter   55/100, finished 279216/364160 ( 77%) of all seeds with 108101 steps/sec\n",
      "Iter   56/100, finished 279679/364160 ( 77%) of all seeds with 104621 steps/sec\n",
      "Iter   57/100, finished 280106/364160 ( 77%) of all seeds with 102800 steps/sec\n",
      "Iter   58/100, finished 280539/364160 ( 77%) of all seeds with 100655 steps/sec\n",
      "Iter   59/100, finished 281013/364160 ( 77%) of all seeds with  97943 steps/sec\n",
      "Iter   60/100, finished 281469/364160 ( 77%) of all seeds with 106492 steps/sec\n",
      "Iter   61/100, finished 281903/364160 ( 77%) of all seeds with 104967 steps/sec\n",
      "Iter   62/100, finished 282325/364160 ( 78%) of all seeds with 100757 steps/sec\n",
      "Iter   63/100, finished 282733/364160 ( 78%) of all seeds with 103207 steps/sec\n",
      "Iter   64/100, finished 283135/364160 ( 78%) of all seeds with 101074 steps/sec\n",
      "Iter   65/100, finished 283546/364160 ( 78%) of all seeds with  92885 steps/sec\n",
      "Iter   66/100, finished 283966/364160 ( 78%) of all seeds with  90589 steps/sec\n",
      "Iter   67/100, finished 284332/364160 ( 78%) of all seeds with  95115 steps/sec\n",
      "Iter   68/100, finished 284737/364160 ( 78%) of all seeds with  99071 steps/sec\n",
      "Iter   69/100, finished 285089/364160 ( 78%) of all seeds with 102323 steps/sec\n",
      "Iter   70/100, finished 285469/364160 ( 78%) of all seeds with  98776 steps/sec\n",
      "Iter   71/100, finished 285812/364160 ( 78%) of all seeds with  92661 steps/sec\n",
      "Iter   72/100, finished 286166/364160 ( 79%) of all seeds with  97188 steps/sec\n",
      "Iter   73/100, finished 286513/364160 ( 79%) of all seeds with  97982 steps/sec\n",
      "Iter   74/100, finished 286843/364160 ( 79%) of all seeds with  96101 steps/sec\n",
      "Iter   75/100, finished 287199/364160 ( 79%) of all seeds with  90883 steps/sec\n",
      "Iter   76/100, finished 287544/364160 ( 79%) of all seeds with  95855 steps/sec\n",
      "Iter   77/100, finished 287888/364160 ( 79%) of all seeds with  94555 steps/sec\n",
      "Iter   78/100, finished 288241/364160 ( 79%) of all seeds with  94322 steps/sec\n",
      "Iter   79/100, finished 288543/364160 ( 79%) of all seeds with  85964 steps/sec\n",
      "Iter   80/100, finished 288821/364160 ( 79%) of all seeds with  87837 steps/sec\n",
      "Iter   81/100, finished 289176/364160 ( 79%) of all seeds with  94302 steps/sec\n",
      "Iter   82/100, finished 289468/364160 ( 79%) of all seeds with  85100 steps/sec\n",
      "Iter   83/100, finished 289726/364160 ( 80%) of all seeds with  94321 steps/sec\n",
      "Iter   84/100, finished 289993/364160 ( 80%) of all seeds with  89566 steps/sec\n",
      "Iter   85/100, finished 290305/364160 ( 80%) of all seeds with  86305 steps/sec\n",
      "Iter   86/100, finished 290611/364160 ( 80%) of all seeds with  87510 steps/sec\n",
      "Iter   87/100, finished 290865/364160 ( 80%) of all seeds with  91433 steps/sec\n",
      "Iter   88/100, finished 291123/364160 ( 80%) of all seeds with  92513 steps/sec\n",
      "Iter   89/100, finished 291384/364160 ( 80%) of all seeds with  88269 steps/sec\n",
      "Iter   90/100, finished 291662/364160 ( 80%) of all seeds with  89095 steps/sec\n",
      "Iter   91/100, finished 291912/364160 ( 80%) of all seeds with  87719 steps/sec\n",
      "Iter   92/100, finished 292200/364160 ( 80%) of all seeds with  89345 steps/sec\n",
      "Iter   93/100, finished 292514/364160 ( 80%) of all seeds with  90234 steps/sec\n",
      "Iter   94/100, finished 292836/364160 ( 80%) of all seeds with  84626 steps/sec\n",
      "Iter   95/100, finished 293144/364160 ( 80%) of all seeds with  80451 steps/sec\n",
      "Iter   96/100, finished 293374/364160 ( 81%) of all seeds with  85436 steps/sec\n",
      "Iter   97/100, finished 293638/364160 ( 81%) of all seeds with  86537 steps/sec\n",
      "Iter   98/100, finished 293970/364160 ( 81%) of all seeds with  87138 steps/sec\n",
      "Iter   99/100, finished 294242/364160 ( 81%) of all seeds with  87462 steps/sec\n",
      "Iter  100/100, finished 294546/364160 ( 81%) of all seeds with  87101 steps/sec\n",
      "\n",
      "Saving ../subjects/992774/predicted_fibers/5f866546f59539cb65d319e2d1268923/fibers.trk\n",
      "Saving ../subjects/992774/predicted_fibers/5f866546f59539cb65d319e2d1268923/config.yml\n"
     ]
    }
   ],
   "source": [
    "tractogram = predict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
