{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "import nipy as ni\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from GPUtil import getFirstAvailable\n",
    "from tensorflow.keras import backend as K\n",
    "from nibabel.streamlines.tck import TckFile\n",
    "from nibabel.streamlines.array_sequence import ArraySequence, concatenate\n",
    "from nibabel.streamlines.tractogram import Tractogram\n",
    "from hashlib import md5\n",
    "from sklearn.preprocessing import normalize\n",
    "from pathos.multiprocessing import Pool\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from time import time\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config):\n",
    "    \"\"\"\n",
    "    seeds: [[x1,y1,z1],...,[xn,yn,zn]]\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    \n",
    "    subject_dir = os.path.join(\"..\", \"subjects\", config[\"subject\"])\n",
    "    \n",
    "    hasher = md5()\n",
    "    for v in config.values():\n",
    "        hasher.update(str(v).encode())\n",
    "    \n",
    "    save_dir = os.path.join(subject_dir, \"predicted_fibers\", hasher.hexdigest())\n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"Predictions with this config have been created already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    print(\"Loading DWI...\")\n",
    "    \n",
    "    dwi_path = os.path.join(subject_dir, config[\"dwi_path\"])\n",
    "    \n",
    "    dwi_img = nib.load(dwi_path)\n",
    "    \n",
    "    affine_original = dwi_img.affine\n",
    "    \n",
    "    dwi_img = nib.funcs.as_closest_canonical(dwi_img)\n",
    "    \n",
    "    affine_canonical = dwi_img.affine\n",
    "    affine_canonical_inv = np.linalg.inv(affine_canonical)\n",
    "    \n",
    "    dwi = dwi_img.get_data()\n",
    "    \n",
    "    print(\"Loading Model...\")\n",
    "    \n",
    "    model_path = os.path.join(config[\"model_dir\"], \"model.h5\")\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    model = load_model(model_path,\n",
    "                       custom_objects={\"negative_log_likelihood\": negative_log_likelihood,\n",
    "                                       \"DistributionLambda\": tfp.layers.DistributionLambda})\n",
    "    \n",
    "    # Define coordinate transforms\n",
    "    \n",
    "    input_shape = model.layers[0].get_output_at(0).get_shape().as_list()[-1]\n",
    "    block_size = int(np.cbrt(input_shape / dwi.shape[-1]))\n",
    "    \n",
    "    def ijk2xyz(indices):\n",
    "        #indices = np.hstack([indices, np.ones((len(indices), 1))])\n",
    "        return affine_canonical.dot(indices.T).T\n",
    "    \n",
    "    def xyz2ijk(coords, snap=False):\n",
    "        #coords = np.hstack([coords, np.ones((len(coords), 1))])\n",
    "        ijk = affine_canonical_inv.dot(coords.T).T\n",
    "        if snap:\n",
    "            ijk = np.round(ijk).astype(int)\n",
    "        return ijk\n",
    "    \n",
    "    # Define Fiber Termination\n",
    "    \n",
    "    class Terminator(object):\n",
    "        def __init__(self):\n",
    "            if \".nii\" in config[\"terminator\"][0]:\n",
    "                scalar_img = nib.load(os.path.join(subject_dir, config[\"terminator\"][0]))\n",
    "                self.scalar = scalar_img.get_data()\n",
    "                aff_inv = np.linalg.inv(scalar_img.affine)\n",
    "                self.xyz2ijk = lambda xyz: np.round(aff_inv.dot(xyz.T).T).astype(int)\n",
    "            else:\n",
    "                raise NotImplementedError # TODO: Implement termination model\n",
    "            self.threshold = config[\"terminator\"][1]\n",
    "        def __call__(self, xyz):\n",
    "            if hasattr(self, \"scalar\"):\n",
    "                ijk = self.xyz2ijk(xyz)\n",
    "                return np.where(self.scalar[ijk[:,0], ijk[:,1], ijk[:,2]] < self.threshold)[0]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    terminator = Terminator()\n",
    "    \n",
    "    print(\"Loading Seeds...\")\n",
    "    \n",
    "    seeds = np.load(os.path.join(subject_dir, config[\"seeds\"]))\n",
    "    # Duplicate seeds for positive and negative starting direction\n",
    "    seeds = np.vstack([seeds, seeds])\n",
    "    \n",
    "    # Define Prior for First Fiber Direction\n",
    "    \n",
    "    class Prior(object):\n",
    "        def __init__(self):\n",
    "            if \".nii\" in config[\"prior\"]:\n",
    "                peak_img = nib.load(os.path.join(subject_dir, config[\"prior\"]))\n",
    "                self.peak = peak_img.get_data()\n",
    "                aff_inv = np.linalg.inv(peak_img.affine)\n",
    "                self.xyz2ijk = lambda xyz: np.round(aff_inv.dot(xyz.T).T).astype(int)\n",
    "            elif \".h5\" in config[\"prior\"]:\n",
    "                raise NotImplementedError # TODO: Implement prior model\n",
    "                \n",
    "        def __call__(self, xyz):\n",
    "            if hasattr(self, \"peak\"):\n",
    "                ijk = self.xyz2ijk(xyz)\n",
    "                # Assuming that seeds have been duplicated\n",
    "                peaks = self.peak[ijk[:,0], ijk[:,1], ijk[:,2]]\n",
    "                peaks[len(ijk)//2:, :3] *= -1\n",
    "                return peaks\n",
    "            elif hasattr(self, \"model\"):\n",
    "                raise NotImplementedError # TODO: Implement prior model\n",
    "        \n",
    "    prior = Prior()\n",
    "    \n",
    "    # Define Interpolation\n",
    "    \n",
    "    def interpolate(ijk):\n",
    "\n",
    "        def inpol_fn(idx):\n",
    "            IDX = np.round(idx).astype(int)\n",
    "\n",
    "            values = np.zeros([3, 3, 3,\n",
    "                               block_size, block_size, block_size,\n",
    "                               dwi.shape[-1]])\n",
    "\n",
    "            for x in range(block_size):\n",
    "                for y in range(block_size):\n",
    "                    for z in range(block_size):\n",
    "                        values[x, y, z,:] = dwi[\n",
    "                            IDX[0] + x - 2 * (block_size // 2) : IDX[0] + x + 1,\n",
    "                            IDX[1] + y - 2 * (block_size // 2) : IDX[1] + y + 1,\n",
    "                            IDX[2] + z - 2 * (block_size // 2) : IDX[2] + z + 1,\n",
    "                            :]\n",
    "                        \n",
    "            fn = RegularGridInterpolator(([-1,0,1],[-1,0,1],[-1,0,1]), values)\n",
    "\n",
    "            d = fn([idx[0]-IDX[0], idx[1]-IDX[1], idx[2]-IDX[2]])[0]\n",
    "            \n",
    "            return d.flatten()\n",
    "        \n",
    "        with Pool(processes=20) as pool:  \n",
    "            return pool.map(inpol_fn, ijk)\n",
    "    \n",
    "    print(\"Initialize Fibers...\")\n",
    "    \n",
    "    assert seeds.shape[-1] == 4   # (x, y, z, 1)\n",
    "    xyz = seeds.reshape(-1, 1, 4) # (fiber, segment, coord) we assume seeds to be in rasmm!\n",
    "    \n",
    "    fiber_idx = np.hstack([np.arange(len(seeds)//2), np.arange(len(seeds)//2)])\n",
    "    fibers = [[] for _ in range(len(seeds)//2)]\n",
    "    \n",
    "    print(\"Start Iteration...\")\n",
    "    \n",
    "    for i in range(config[\"max_steps\"]):\n",
    "        t0 = time()\n",
    "        \n",
    "        if config[\"interpolate_dwi\"]:\n",
    "            d = interpolate(ijk)\n",
    "        else:\n",
    "            ijk = xyz2ijk(xyz[:,-1,:], snap=True) # Get coords of latest segement for each fiber \n",
    "\n",
    "            d = np.zeros([len(ijk), block_size, block_size, block_size, dwi.shape[-1]])\n",
    "            \n",
    "            for ii, idx in enumerate(ijk):\n",
    "                d[ii] = dwi[idx[0] - (block_size // 2) : idx[0] + (block_size // 2) + 1,\n",
    "                            idx[1] - (block_size // 2) : idx[1] + (block_size // 2) + 1,\n",
    "                            idx[2] - (block_size // 2) : idx[2] + (block_size // 2) + 1,\n",
    "                        :]\n",
    "                \n",
    "            d = d.reshape(-1, dwi.shape[-1] * block_size**3)\n",
    "        \n",
    "        if i == 0:\n",
    "            vin = prior(xyz[:,0,:])\n",
    "        else:\n",
    "            vin = vout.copy()\n",
    "        \n",
    "        if config[\"predict_fn\"] == \"mean\":\n",
    "            vout = model(np.hstack([vin,d])).mean().numpy()\n",
    "            vout = normalize(vout) # Careful, the FvM mean is not a unit vector!\n",
    "        else:\n",
    "            vout = model(np.hstack([vin,d])).sample().numpy() # Samples are unit length, though!\n",
    "        \n",
    "        rout = (xyz[:, -1, :3] + config[\"step_size\"] * vout)\n",
    "        rout = np.hstack([rout, np.ones((len(rout), 1))]).reshape(-1, 1, 4)\n",
    "        \n",
    "        xyz = np.concatenate([xyz, rout], axis=1)\n",
    "        \n",
    "        terminal_indices = terminator(xyz[:, -1, :]) # Check latest points for termination\n",
    "\n",
    "        for idx in terminal_indices:\n",
    "            gidx = fiber_idx[idx]\n",
    "            # Other end not yet added\n",
    "            if not fibers[gidx]:\n",
    "                fibers[gidx].append(xyz[idx, :, :3])\n",
    "            # Other end already added\n",
    "            else:\n",
    "                this_end = xyz[idx, :, :3]\n",
    "                other_end = fibers[gidx][0]\n",
    "                merged_fiber = np.vstack([np.flip(this_end[1:], axis=0), other_end]) # stitch ends together\n",
    "                fibers[gidx] = [merged_fiber]\n",
    "                \n",
    "        xyz = np.delete(xyz, terminal_indices, axis=0)\n",
    "        vout = np.delete(vout, terminal_indices, axis=0)\n",
    "        fiber_idx = np.delete(fiber_idx, terminal_indices)\n",
    "        \n",
    "        print(\"Iter {:4d}/{}, finished {:5d}/{:5d} ({:3.0f}%) of all seeds with {:6.0f} steps/sec\".format(\n",
    "            (i+1), config[\"max_steps\"], len(seeds)-len(fiber_idx), len(seeds),\n",
    "            100*(1-len(fiber_idx)/len(seeds)), len(vin) / (time() - t0), end=\"\\r\")\n",
    "        )\n",
    "        if len(fiber_idx) == 0:\n",
    "            break\n",
    "    \n",
    "    # Include unfinished fibers:\n",
    "    \n",
    "    for idx, gidx in enumerate(fiber_idx):\n",
    "        if not fibers[gidx]:\n",
    "            fibers[gidx].append(xyz[idx, :, :3])\n",
    "        else:\n",
    "            this_end = xyz[idx, :, :3]\n",
    "            other_end = fibers[gidx][0]\n",
    "            merged_fiber = np.vstack([np.flip(this_end[1:], axis=0), other_end])\n",
    "            fibers[gidx] = [merged_fiber]\n",
    "    \n",
    "    # Save Result\n",
    "    \n",
    "    fibers = [f[0] for f in fibers]\n",
    "    \n",
    "    tractogram = Tractogram(\n",
    "        streamlines=ArraySequence(fibers),\n",
    "        affine_to_rasmm=np.eye(4)\n",
    "    )\n",
    "    \n",
    "    tractogram.apply_affine(affine_original.dot(affine_canonical_inv))\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    fiber_path = os.path.join(save_dir, \"fibers.trk\")\n",
    "    print(\"\\nSaving {}\".format(fiber_path))\n",
    "    nib.streamlines.save(tractogram, fiber_path)\n",
    "\n",
    "    config_path = os.path.join(save_dir, \"config.yml\")\n",
    "    print(\"Saving {}\".format(config_path))\n",
    "    with open(config_path, \"w\") as file:\n",
    "        yaml.dump(config, file, default_flow_style=False) \n",
    "    \n",
    "    return tractogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    subject=\"ismrm_rpe\",\n",
    "    model_dir=\"../models/entrack_conditional/07371985349211c04c0aee0a48c84a9e\",\n",
    "    dwi_path=\"fod_norm_125.nii.gz\",\n",
    "    prior=\"peaks_125.nii.gz\",\n",
    "    seeds=\"wm_seeds.npy\",\n",
    "    terminator=[\"fa_125.nii.gz\", 0.3],\n",
    "    predict_fn=\"mean\",\n",
    "    interpolate_dwi=False,\n",
    "    step_size=0.5,\n",
    "    max_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DWI...\n",
      "Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1017 22:18:51.955837 140433845106432 deprecation.py:323] From /local/home/vwegmayr/miniconda2/envs/thesis/lib/python3.6/site-packages/tensorflow_probability/python/distributions/von_mises_fisher.py:312: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Seeds...\n",
      "Initialize Fibers...\n",
      "Start Iteration...\n",
      "Iter    1/100, finished    24/ 9136 (  0%) of all seeds with  13019 steps/sec\n",
      "Iter    2/100, finished    53/ 9136 (  1%) of all seeds with  61476 steps/sec\n",
      "Iter    3/100, finished    71/ 9136 (  1%) of all seeds with  62060 steps/sec\n",
      "Iter    4/100, finished    92/ 9136 (  1%) of all seeds with  39239 steps/sec\n",
      "Iter    5/100, finished   123/ 9136 (  1%) of all seeds with  35166 steps/sec\n",
      "Iter    6/100, finished   132/ 9136 (  1%) of all seeds with  63949 steps/sec\n",
      "Iter    7/100, finished   174/ 9136 (  2%) of all seeds with  49932 steps/sec\n",
      "Iter    8/100, finished   208/ 9136 (  2%) of all seeds with  64406 steps/sec\n",
      "Iter    9/100, finished   228/ 9136 (  2%) of all seeds with  42967 steps/sec\n",
      "Iter   10/100, finished   309/ 9136 (  3%) of all seeds with  36693 steps/sec\n",
      "Iter   11/100, finished   357/ 9136 (  4%) of all seeds with  66160 steps/sec\n",
      "Iter   12/100, finished   438/ 9136 (  5%) of all seeds with  62363 steps/sec\n",
      "Iter   13/100, finished   524/ 9136 (  6%) of all seeds with  35534 steps/sec\n",
      "Iter   14/100, finished   612/ 9136 (  7%) of all seeds with  62311 steps/sec\n",
      "Iter   15/100, finished   737/ 9136 (  8%) of all seeds with  53283 steps/sec\n",
      "Iter   16/100, finished   860/ 9136 (  9%) of all seeds with  35620 steps/sec\n",
      "Iter   17/100, finished   990/ 9136 ( 11%) of all seeds with  37143 steps/sec\n",
      "Iter   18/100, finished  1154/ 9136 ( 13%) of all seeds with  33853 steps/sec\n",
      "Iter   19/100, finished  1318/ 9136 ( 14%) of all seeds with  30575 steps/sec\n",
      "Iter   20/100, finished  1471/ 9136 ( 16%) of all seeds with  35413 steps/sec\n",
      "Iter   21/100, finished  1646/ 9136 ( 18%) of all seeds with  60267 steps/sec\n",
      "Iter   22/100, finished  1803/ 9136 ( 20%) of all seeds with  33304 steps/sec\n",
      "Iter   23/100, finished  1988/ 9136 ( 22%) of all seeds with  29953 steps/sec\n",
      "Iter   24/100, finished  2133/ 9136 ( 23%) of all seeds with  60235 steps/sec\n",
      "Iter   25/100, finished  2300/ 9136 ( 25%) of all seeds with  58660 steps/sec\n",
      "Iter   26/100, finished  2483/ 9136 ( 27%) of all seeds with  56635 steps/sec\n",
      "Iter   27/100, finished  2619/ 9136 ( 29%) of all seeds with  30928 steps/sec\n",
      "Iter   28/100, finished  2780/ 9136 ( 30%) of all seeds with  59341 steps/sec\n",
      "Iter   29/100, finished  2902/ 9136 ( 32%) of all seeds with  46122 steps/sec\n",
      "Iter   30/100, finished  3051/ 9136 ( 33%) of all seeds with  40722 steps/sec\n",
      "Iter   31/100, finished  3197/ 9136 ( 35%) of all seeds with  52690 steps/sec\n",
      "Iter   32/100, finished  3340/ 9136 ( 37%) of all seeds with  49910 steps/sec\n",
      "Iter   33/100, finished  3492/ 9136 ( 38%) of all seeds with  28425 steps/sec\n",
      "Iter   34/100, finished  3618/ 9136 ( 40%) of all seeds with  31414 steps/sec\n",
      "Iter   35/100, finished  3775/ 9136 ( 41%) of all seeds with  26542 steps/sec\n",
      "Iter   36/100, finished  3931/ 9136 ( 43%) of all seeds with  35299 steps/sec\n",
      "Iter   37/100, finished  4074/ 9136 ( 45%) of all seeds with  27681 steps/sec\n",
      "Iter   38/100, finished  4226/ 9136 ( 46%) of all seeds with  12812 steps/sec\n",
      "Iter   39/100, finished  4343/ 9136 ( 48%) of all seeds with  51904 steps/sec\n",
      "Iter   40/100, finished  4479/ 9136 ( 49%) of all seeds with  48967 steps/sec\n",
      "Iter   41/100, finished  4622/ 9136 ( 51%) of all seeds with  28504 steps/sec\n",
      "Iter   42/100, finished  4765/ 9136 ( 52%) of all seeds with  36831 steps/sec\n",
      "Iter   43/100, finished  4911/ 9136 ( 54%) of all seeds with  39401 steps/sec\n",
      "Iter   44/100, finished  5029/ 9136 ( 55%) of all seeds with  51463 steps/sec\n",
      "Iter   45/100, finished  5161/ 9136 ( 56%) of all seeds with  23772 steps/sec\n",
      "Iter   46/100, finished  5283/ 9136 ( 58%) of all seeds with  17091 steps/sec\n",
      "Iter   47/100, finished  5408/ 9136 ( 59%) of all seeds with  42609 steps/sec\n",
      "Iter   48/100, finished  5534/ 9136 ( 61%) of all seeds with  44593 steps/sec\n",
      "Iter   49/100, finished  5646/ 9136 ( 62%) of all seeds with  52240 steps/sec\n",
      "Iter   50/100, finished  5780/ 9136 ( 63%) of all seeds with  55725 steps/sec\n",
      "Iter   51/100, finished  5897/ 9136 ( 65%) of all seeds with  54853 steps/sec\n",
      "Iter   52/100, finished  6017/ 9136 ( 66%) of all seeds with  50176 steps/sec\n",
      "Iter   53/100, finished  6130/ 9136 ( 67%) of all seeds with  45650 steps/sec\n",
      "Iter   54/100, finished  6243/ 9136 ( 68%) of all seeds with  32667 steps/sec\n",
      "Iter   55/100, finished  6344/ 9136 ( 69%) of all seeds with  32805 steps/sec\n",
      "Iter   56/100, finished  6469/ 9136 ( 71%) of all seeds with  34911 steps/sec\n",
      "Iter   57/100, finished  6570/ 9136 ( 72%) of all seeds with  38403 steps/sec\n",
      "Iter   58/100, finished  6685/ 9136 ( 73%) of all seeds with  36038 steps/sec\n",
      "Iter   59/100, finished  6775/ 9136 ( 74%) of all seeds with  45522 steps/sec\n",
      "Iter   60/100, finished  6867/ 9136 ( 75%) of all seeds with  42670 steps/sec\n",
      "Iter   61/100, finished  6953/ 9136 ( 76%) of all seeds with  44049 steps/sec\n",
      "Iter   62/100, finished  7048/ 9136 ( 77%) of all seeds with  39502 steps/sec\n",
      "Iter   63/100, finished  7124/ 9136 ( 78%) of all seeds with  35917 steps/sec\n",
      "Iter   64/100, finished  7199/ 9136 ( 79%) of all seeds with  29636 steps/sec\n",
      "Iter   65/100, finished  7251/ 9136 ( 79%) of all seeds with  24755 steps/sec\n",
      "Iter   66/100, finished  7320/ 9136 ( 80%) of all seeds with  33092 steps/sec\n",
      "Iter   67/100, finished  7385/ 9136 ( 81%) of all seeds with  36421 steps/sec\n",
      "Iter   68/100, finished  7454/ 9136 ( 82%) of all seeds with  37472 steps/sec\n",
      "Iter   69/100, finished  7510/ 9136 ( 82%) of all seeds with  36382 steps/sec\n",
      "Iter   70/100, finished  7565/ 9136 ( 83%) of all seeds with  39244 steps/sec\n",
      "Iter   71/100, finished  7622/ 9136 ( 83%) of all seeds with  37231 steps/sec\n",
      "Iter   72/100, finished  7673/ 9136 ( 84%) of all seeds with  34073 steps/sec\n",
      "Iter   73/100, finished  7718/ 9136 ( 84%) of all seeds with  31536 steps/sec\n",
      "Iter   74/100, finished  7771/ 9136 ( 85%) of all seeds with  34746 steps/sec\n",
      "Iter   75/100, finished  7819/ 9136 ( 86%) of all seeds with  31868 steps/sec\n",
      "Iter   76/100, finished  7863/ 9136 ( 86%) of all seeds with  24083 steps/sec\n",
      "Iter   77/100, finished  7902/ 9136 ( 86%) of all seeds with  28972 steps/sec\n",
      "Iter   78/100, finished  7938/ 9136 ( 87%) of all seeds with  27791 steps/sec\n",
      "Iter   79/100, finished  7973/ 9136 ( 87%) of all seeds with  27362 steps/sec\n",
      "Iter   80/100, finished  8004/ 9136 ( 88%) of all seeds with  27360 steps/sec\n",
      "Iter   81/100, finished  8030/ 9136 ( 88%) of all seeds with  28043 steps/sec\n",
      "Iter   82/100, finished  8054/ 9136 ( 88%) of all seeds with  24475 steps/sec\n",
      "Iter   83/100, finished  8079/ 9136 ( 88%) of all seeds with  17954 steps/sec\n",
      "Iter   84/100, finished  8111/ 9136 ( 89%) of all seeds with  18938 steps/sec\n",
      "Iter   85/100, finished  8138/ 9136 ( 89%) of all seeds with  19995 steps/sec\n",
      "Iter   86/100, finished  8164/ 9136 ( 89%) of all seeds with  25155 steps/sec\n",
      "Iter   87/100, finished  8192/ 9136 ( 90%) of all seeds with  25307 steps/sec\n",
      "Iter   88/100, finished  8225/ 9136 ( 90%) of all seeds with  24399 steps/sec\n",
      "Iter   89/100, finished  8252/ 9136 ( 90%) of all seeds with  23447 steps/sec\n",
      "Iter   90/100, finished  8279/ 9136 ( 91%) of all seeds with  24210 steps/sec\n",
      "Iter   91/100, finished  8302/ 9136 ( 91%) of all seeds with  24462 steps/sec\n",
      "Iter   92/100, finished  8331/ 9136 ( 91%) of all seeds with  23490 steps/sec\n",
      "Iter   93/100, finished  8354/ 9136 ( 91%) of all seeds with  23374 steps/sec\n",
      "Iter   94/100, finished  8375/ 9136 ( 92%) of all seeds with  22010 steps/sec\n",
      "Iter   95/100, finished  8397/ 9136 ( 92%) of all seeds with  20332 steps/sec\n",
      "Iter   96/100, finished  8421/ 9136 ( 92%) of all seeds with  21565 steps/sec\n",
      "Iter   97/100, finished  8445/ 9136 ( 92%) of all seeds with  21238 steps/sec\n",
      "Iter   98/100, finished  8471/ 9136 ( 93%) of all seeds with  19482 steps/sec\n",
      "Iter   99/100, finished  8497/ 9136 ( 93%) of all seeds with  18981 steps/sec\n",
      "Iter  100/100, finished  8525/ 9136 ( 93%) of all seeds with  18174 steps/sec\n",
      "\n",
      "Saving ../subjects/ismrm_rpe/predicted_fibers/da4127c8f9ee7dd42f31bece79b801a4/fibers.trk\n",
      "Saving ../subjects/ismrm_rpe/predicted_fibers/da4127c8f9ee7dd42f31bece79b801a4/config.yml\n"
     ]
    }
   ],
   "source": [
    "tractogram = predict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi_image = nib.load(\"../subjects/ismrm_rpe/fod_norm_125.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi_ni = ni.load_image(\"../subjects/ismrm_rpe/fod_norm_125.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.875, -2.   ,  3.375,  1.   ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwi_ni.coordmap([1,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.25 ,  0.   ,  0.   ,  0.375],\n",
       "       [ 0.   , -1.25 ,  0.   ,  0.5  ],\n",
       "       [ 0.   ,  0.   ,  1.25 , -0.375],\n",
       "       [ 0.   ,  0.   ,  0.   ,  1.   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwi_image.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi_canonical = nib.funcs.as_closest_canonical(dwi_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.25 ,    0.   ,    0.   , -178.375],\n",
       "       [   0.   ,    1.25 ,    0.   , -214.5  ],\n",
       "       [   0.   ,    0.   ,    1.25 ,   -0.375],\n",
       "       [   0.   ,    0.   ,    0.   ,    1.   ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwi_canonical.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1.,    0.,    0., -178.],\n",
       "       [   0.,   -1.,    0., -214.],\n",
       "       [   0.,    0.,    1.,    0.],\n",
       "       [   0.,    0.,    0.,    1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwi_image.affine.dot(np.linalg.inv(dwi_canonical.affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 173, 144, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwi_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
