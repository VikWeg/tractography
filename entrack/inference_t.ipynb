{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import yaml\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "import nipy as ni\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from GPUtil import getFirstAvailable\n",
    "from tensorflow.keras import backend as K\n",
    "from nibabel.streamlines.trk import TrkFile\n",
    "from nibabel.streamlines.array_sequence import ArraySequence, concatenate\n",
    "from nibabel.streamlines.tractogram import Tractogram\n",
    "from hashlib import md5\n",
    "from sklearn.preprocessing import normalize\n",
    "from pathos.multiprocessing import Pool\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from time import time\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config):\n",
    "    \"\"\"\n",
    "    seeds: [[x1,y1,z1],...,[xn,yn,zn]]\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    \n",
    "    subject_dir = os.path.join(\"..\", \"subjects\", config[\"subject\"])\n",
    "    \n",
    "    hasher = md5()\n",
    "    for v in config.values():\n",
    "        hasher.update(str(v).encode())\n",
    "    \n",
    "    save_dir = os.path.join(subject_dir, \"predicted_fibers\", hasher.hexdigest())\n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"Predictions with this config have been created already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    print(\"Loading DWI...\")\n",
    "    \n",
    "    dwi_path = os.path.join(subject_dir, config[\"dwi_path\"])\n",
    "    dwi_img = nib.load(dwi_path)\n",
    "    dwi_aff1 = dwi_img.affine\n",
    "\n",
    "    dwi_img = nib.funcs.as_closest_canonical(dwi_img)\n",
    "    dwi_aff2 = dwi_img.affine\n",
    "    dwi_affi2 = np.linalg.inv(dwi_aff2)\n",
    "    dwi = dwi_img.get_data()\n",
    "    \n",
    "    print(\"Loading Model...\")\n",
    "    \n",
    "    model_path = os.path.join(config[\"model_dir\"], \"model.h5\")\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    model = load_model(model_path,\n",
    "                       custom_objects={\"negative_log_likelihood\": negative_log_likelihood,\n",
    "                                       \"DistributionLambda\": tfp.layers.DistributionLambda})\n",
    "    \n",
    "    # Define coordinate transforms\n",
    "    \n",
    "    input_shape = model.layers[0].get_output_at(0).get_shape().as_list()[-1]\n",
    "    block_size = int(np.cbrt(input_shape / dwi.shape[-1]))\n",
    "    \n",
    "    def xyz2ijk(coords, snap=False, shift=False):\n",
    "        ijk = dwi_affi2.dot(coords.T).T\n",
    "        if snap:\n",
    "            ijk = np.round(ijk).astype(int)\n",
    "        return ijk\n",
    "    \n",
    "    # Define Fiber Termination\n",
    "    \n",
    "    class Terminator(object):\n",
    "        def __init__(self):\n",
    "            if \".nii\" in config[\"terminator\"][0]:\n",
    "                scalar_img = nib.load(os.path.join(subject_dir, config[\"terminator\"][0]))\n",
    "                self.scalar = scalar_img.get_data()\n",
    "                affi = np.linalg.inv(scalar_img.affine)\n",
    "                self.xyz2ijk = lambda xyz: np.round(affi.dot(xyz.T).T).astype(int)\n",
    "            else:\n",
    "                raise NotImplementedError # TODO: Implement termination model\n",
    "            self.threshold = config[\"terminator\"][1]\n",
    "        def __call__(self, xyz):\n",
    "            if hasattr(self, \"scalar\"):\n",
    "                ijk = self.xyz2ijk(xyz)\n",
    "                return np.where(self.scalar[ijk[:,0], ijk[:,1], ijk[:,2]] < self.threshold)[0]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    terminator = Terminator()\n",
    "    \n",
    "    print(\"Loading Seeds...\")\n",
    "    \n",
    "    seed_file = nib.streamlines.load(config[\"seed_path\"])\n",
    "    seeds = seed_file.tractogram.streamlines.data\n",
    "    seeds = np.vstack([seeds, seeds])  # Duplicate seeds for positive and negative starting direction\n",
    "    seeds = np.hstack([seeds, np.ones([len(seeds), 1])]) # add affine dimension\n",
    "    assert seeds.shape[-1] == 4   # (x, y, z, 1)\n",
    "    \n",
    "    # Define Prior for First Fiber Direction\n",
    "    \n",
    "    class Prior(object):\n",
    "        def __init__(self):\n",
    "            if \".nii\" in config[\"prior\"]:\n",
    "                peak_img = nib.load(os.path.join(subject_dir, config[\"prior\"]))\n",
    "                self.peak = peak_img.get_data()\n",
    "                affi = np.linalg.inv(peak_img.affine)\n",
    "                self.xyz2ijk = lambda xyz: np.round(affi.dot(xyz.T).T).astype(int)\n",
    "            elif \".h5\" in config[\"prior\"]:\n",
    "                raise NotImplementedError # TODO: Implement prior model\n",
    "                \n",
    "        def __call__(self, xyz):\n",
    "            if hasattr(self, \"peak\"):\n",
    "                ijk = self.xyz2ijk(xyz)\n",
    "                # Assuming that seeds have been duplicated\n",
    "                peaks = self.peak[ijk[:,0], ijk[:,1], ijk[:,2]]\n",
    "                peaks[len(ijk)//2:, :] *= -1\n",
    "                return normalize(peaks)\n",
    "            elif hasattr(self, \"model\"):\n",
    "                raise NotImplementedError # TODO: Implement prior model\n",
    "        \n",
    "    prior = Prior()\n",
    "    \n",
    "    print(\"Initialize Fibers...\")\n",
    "    \n",
    "    xyz = seeds.reshape(-1, 1, 4) # (fiber, segment, coord) we assume seeds to be in rasmm!\n",
    "    \n",
    "    fiber_idx = np.hstack([np.arange(len(seeds)//2), np.arange(len(seeds)//2)])\n",
    "    fibers = [[] for _ in range(len(seeds)//2)]\n",
    "    \n",
    "    print(\"Start Iteration...\")\n",
    "    \n",
    "    for i in range(config[\"max_steps\"]):\n",
    "        t0 = time()\n",
    "        \n",
    "        ijk = xyz2ijk(xyz[:,-1,:], snap=True, shift=True) # Get coords of latest segement for each fiber \n",
    "\n",
    "        d = np.zeros([len(ijk), block_size, block_size, block_size, dwi.shape[-1]])\n",
    "\n",
    "        for ii, idx in enumerate(ijk):\n",
    "            d[ii] = dwi[idx[0] - (block_size // 2) : idx[0] + (block_size // 2) + 1,\n",
    "                        idx[1] - (block_size // 2) : idx[1] + (block_size // 2) + 1,\n",
    "                        idx[2] - (block_size // 2) : idx[2] + (block_size // 2) + 1,\n",
    "                    :]\n",
    "        d = d.reshape(-1, dwi.shape[-1] * block_size**3)\n",
    "        \n",
    "        if i == 0:\n",
    "            vin = prior(xyz[:,0,:])\n",
    "        else:\n",
    "            vin = vout.copy()\n",
    "        \n",
    "        chunk_size = 2**15 # 32768\n",
    "        n_chunks = np.ceil(len(vin) / chunk_size).astype(int)\n",
    "        \n",
    "        inputs = np.hstack([vin,d])\n",
    "        vout = np.zeros([len(vin), 3])\n",
    "        for chunk in range(n_chunks):\n",
    "            input_chunk = inputs[chunk * chunk_size : (chunk + 1) * chunk_size]\n",
    "            if config[\"predict_fn\"] == \"mean\":\n",
    "                v = model(input_chunk).mean().numpy()\n",
    "                v = normalize(v) # Careful, the FvM mean is not a unit vector!\n",
    "            else:\n",
    "                v = model(input_chunk).sample().numpy() # Samples are unit length, though!\n",
    "            vout[chunk * chunk_size : (chunk + 1) * chunk_size] = v\n",
    "           \n",
    "        rout = (xyz[:, -1, :3] + config[\"step_size\"] * vout)\n",
    "        rout = np.hstack([rout, np.ones((len(rout), 1))]).reshape(-1, 1, 4)\n",
    "        \n",
    "        xyz = np.concatenate([xyz, rout], axis=1)\n",
    "        \n",
    "        terminal_indices = terminator(xyz[:, -1, :]) # Check latest points for termination\n",
    "\n",
    "        for idx in terminal_indices:\n",
    "            gidx = fiber_idx[idx]\n",
    "            # Other end not yet added\n",
    "            if not fibers[gidx]:\n",
    "                fibers[gidx].append(xyz[idx, :, :3])\n",
    "            # Other end already added\n",
    "            else:\n",
    "                this_end = xyz[idx, :, :3]\n",
    "                other_end = fibers[gidx][0]\n",
    "                merged_fiber = np.vstack([np.flip(this_end[1:], axis=0), other_end]) # stitch ends together\n",
    "                fibers[gidx] = [merged_fiber]\n",
    "                \n",
    "        xyz = np.delete(xyz, terminal_indices, axis=0)\n",
    "        vout = np.delete(vout, terminal_indices, axis=0)\n",
    "        fiber_idx = np.delete(fiber_idx, terminal_indices)\n",
    "        \n",
    "        print(\"Iter {:4d}/{}, finished {:5d}/{:5d} ({:3.0f}%) of all seeds with {:6.0f} steps/sec\".format(\n",
    "            (i+1), config[\"max_steps\"], len(seeds)-len(fiber_idx), len(seeds),\n",
    "            100*(1-len(fiber_idx)/len(seeds)), len(vin) / (time() - t0)),\n",
    "            end=\"\\r\"\n",
    "        )\n",
    "        \n",
    "        if len(fiber_idx) == 0:\n",
    "            break\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    # Include unfinished fibers:\n",
    "    \n",
    "    for idx, gidx in enumerate(fiber_idx):\n",
    "        if not fibers[gidx]:\n",
    "            fibers[gidx].append(xyz[idx, :, :3])\n",
    "        else:\n",
    "            this_end = xyz[idx, :, :3]\n",
    "            other_end = fibers[gidx][0]\n",
    "            merged_fiber = np.vstack([np.flip(this_end[1:], axis=0), other_end])\n",
    "            fibers[gidx] = [merged_fiber]\n",
    "    \n",
    "    # Save Result\n",
    "    \n",
    "    fibers = [f[0] for f in fibers]\n",
    "    \n",
    "    tractogram = Tractogram(\n",
    "        streamlines=ArraySequence(fibers),\n",
    "        affine_to_rasmm=np.eye(4)\n",
    "    )\n",
    "    \n",
    "    # tractogram.apply_affine(affine_original.dot(affine_canonical_inv))\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    fiber_path = os.path.join(save_dir, \"fibers.trk\")\n",
    "    print(\"\\nSaving {}\".format(fiber_path))\n",
    "    TrkFile(tractogram, seed_file.header).save(fiber_path)\n",
    "    #nib.streamlines.save(tractogram, fiber_path)\n",
    "\n",
    "    config_path = os.path.join(save_dir, \"config.yml\")\n",
    "    print(\"Saving {}\".format(config_path))\n",
    "    with open(config_path, \"w\") as file:\n",
    "        yaml.dump(config, file, default_flow_style=False) \n",
    "    \n",
    "    return tractogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    subject=\"ismrm_gt\",\n",
    "    model_dir=\"../models/entrack_conditional/f57290355caf38c41d6aeca1c7dc2091\",\n",
    "    dwi_path=\"fod_norm_125.nii.gz\",\n",
    "    prior=\"tensor_V1_125.nii.gz\",\n",
    "    seed_path=\"../scoring/scoring_data/seeds/seeds_from_wm_100.trk\",\n",
    "    terminator=[\"wm_mask_125.nii.gz\", 0.1],\n",
    "    predict_fn=\"mean\",\n",
    "    step_size=0.25,\n",
    "    max_steps=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DWI...\n",
      "Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1024 08:29:26.110945 140356650682112 deprecation.py:323] From /local/home/vwegmayr/miniconda2/envs/thesis/lib/python3.6/site-packages/tensorflow_probability/python/distributions/von_mises_fisher.py:312: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Seeds...\n",
      "Initialize Fibers...\n",
      "Start Iteration...\n",
      "Iter  259/400, finished 476493/534844 ( 89%) of all seeds with   4487 steps/sec\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7aa7bb5546f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtractogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4d00d91850d0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mthis_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxyz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mother_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfibers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mmerged_fiber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stitch ends together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mfibers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgidx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmerged_fiber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/thesis/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \"\"\"\n\u001b[1;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tractogram = predict(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
