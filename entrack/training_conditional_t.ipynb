{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import nibabel as nib\n",
    "import nipy as ni\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from hashlib import md5\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import (Input, Reshape, Dropout, BatchNormalization, Lambda, Dense)\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from multiprocessing import cpu_count\n",
    "from GPUtil import getFirstAvailable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conditional_samples(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sample_dir, batch_size=256, istraining=True, max_n_samples=np.inf):\n",
    "        \"\"\"\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.istraining = istraining\n",
    "\n",
    "        samples = np.load(os.path.join(sample_dir, \"samples.npz\"))\n",
    "        \n",
    "        self.inputs = samples[\"inputs\"]\n",
    "        self.outputs = samples[\"outputs\"]\n",
    "        \n",
    "        assert len(self.inputs) == len(self.outputs)\n",
    "        \n",
    "        self.inputs = self.inputs[:min(max_n_samples, len(self.inputs))]\n",
    "        self.outputs = self.outputs[:min(max_n_samples, len(self.outputs))]\n",
    "        \n",
    "        self.n_samples = len(self.inputs)\n",
    "        \n",
    "        assert self.n_samples > 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.istraining:\n",
    "            return self.n_samples // self.batch_size # drop remainder\n",
    "        else:\n",
    "             return np.ceil(self.n_samples / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_batch = np.vstack(self.inputs[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        y_batch = np.vstack(self.outputs[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        \n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    hasher = md5()\n",
    "    for v in config.values():\n",
    "        hasher.update(str(v).encode())\n",
    "    \n",
    "    save_dir = os.path.join(\"..\", \"models\", config[\"model_name\"], hasher.hexdigest())\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"This model config has been trained already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    # Define Model Function and Loss\n",
    "    \n",
    "    train_path = os.path.join(config[\"train_dir\"], \"samples.npz\")\n",
    "    \n",
    "    input_shape = np.load(train_path)[\"inputs\"].shape[1:]\n",
    "    inputs = Input(shape=input_shape, name=\"inputs\")\n",
    "\n",
    "    def model_fn(inputs):\n",
    "\n",
    "        x = Dense(1024, activation=\"relu\")(inputs)\n",
    "\n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "        mu = Dense(512, activation=\"relu\")(x)\n",
    "        mu = Dense(3, activation=\"linear\")(mu)\n",
    "        mu = Lambda(lambda t: K.l2_normalize(t, axis=-1), name=\"mu\")(mu)\n",
    "\n",
    "        kappa = Dense(512, activation=\"relu\")(x)\n",
    "        kappa = Dense(1, activation=\"relu\")(kappa)\n",
    "        kappa = Lambda(lambda t: K.squeeze(t, 1), name=\"kappa\")(kappa) \n",
    "        \n",
    "        return tfp.layers.DistributionLambda(\n",
    "            make_distribution_fn=lambda params: tfd.VonMisesFisher(mean_direction=params[0],\n",
    "                                                                   concentration=params[1]),\n",
    "            convert_to_tensor_fn=tfd.Distribution.mean\n",
    "        )([mu, kappa])\n",
    "\n",
    "    model = tf.keras.Model(inputs, model_fn(inputs), name=config[\"model_name\"])\n",
    "    model.summary()\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    # Run Training\n",
    "\n",
    "    train_seq = conditional_samples(config[\"train_dir\"], config[\"batch_size\"],\n",
    "                                    max_n_samples=config[\"max_n_samples\"])\n",
    "    #eval_seq = conditional_samples(config[\"eval_dir\"], istraining=False)\n",
    "    try:\n",
    "        no_exception = True\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss=negative_log_likelihood\n",
    "        )\n",
    "        train_history = model.fit_generator(\n",
    "            train_seq,\n",
    "            epochs=config[\"epochs\"],\n",
    "            #validation_data=eval_seq,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=save_dir,\n",
    "                            write_graph=False,\n",
    "                            update_freq=5*config[\"batch_size\"],\n",
    "                            profile_batch=0),\n",
    "                #ModelCheckpoint(os.path.join(save_dir, \"weights.{epoch:02d}-{val_loss:.2f}.h5\"),\n",
    "                #                save_best_only=True),\n",
    "                # EarlyStopping(min_delta=0.05, patience=10, restore_best_weights=True, verbose=1)\n",
    "            ],\n",
    "            max_queue_size=2*config[\"batch_size\"],\n",
    "            use_multiprocessing=True,\n",
    "            workers=cpu_count()\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        os.rename(save_dir, save_dir + \"_stopped\")\n",
    "        save_dir = save_dir + \"_stopped\"\n",
    "    except Exception as e:\n",
    "        shutil.rmtree(save_dir)\n",
    "        no_exception = False\n",
    "        raise e\n",
    "    finally:\n",
    "        if no_exception:\n",
    "            config_path = os.path.join(save_dir, \"config\" + \".yml\")\n",
    "            print(\"Saving {}\".format(config_path))\n",
    "            with open(config_path, \"w\") as file:\n",
    "                yaml.dump(config, file, default_flow_style=False)           \n",
    "\n",
    "            model_path = os.path.join(save_dir, \"model.h5\")\n",
    "            print(\"Saving {}\".format(model_path))\n",
    "            model.save(model_path)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    model_name=\"entrack_conditional\",\n",
    "    train_dir=\"../subjects/992774/samples/2392afe9c0d78631915fa44f37a48d90\",\n",
    "    #eval_dir=\"../subjects/992774/samples/0c7f800138a28ef0840333930880d857\",\n",
    "    max_n_samples=np.inf,\n",
    "    batch_size = 256,\n",
    "    epochs = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1021 19:00:22.246206 140129666033408 deprecation.py:323] From /local/home/vwegmayr/miniconda2/envs/thesis/lib/python3.6/site-packages/tensorflow_probability/python/distributions/von_mises_fisher.py:312: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"entrack_conditional\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 408)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         418816      inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         1049600     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            1539        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            513         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mu (Lambda)                     (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "kappa (Lambda)                  (None,)              0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "distribution_lambda (Distributi ((None, 3), (None, 3 0           mu[0][0]                         \n",
      "                                                                 kappa[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,520,068\n",
      "Trainable params: 2,520,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 1.1944\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 1s 27ms/step - loss: -0.7369\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 1s 28ms/step - loss: -1.0005\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 1s 26ms/step - loss: -0.9059\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 1s 26ms/step - loss: -1.2239\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 1s 25ms/step - loss: -1.2550\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 1s 28ms/step - loss: -1.2310\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 1s 29ms/step - loss: -1.0952\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 1s 22ms/step - loss: -1.3581\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 1s 31ms/step - loss: -1.4778\n",
      "Saving ../models/entrack_conditional/789acc8e14db470e0696d17b76b9a0a6/config.yml\n",
      "Saving ../models/entrack_conditional/789acc8e14db470e0696d17b76b9a0a6/model.h5\n"
     ]
    }
   ],
   "source": [
    "model = train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
