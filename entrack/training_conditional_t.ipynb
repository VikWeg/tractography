{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import nibabel as nib\n",
    "import nipy as ni\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from hashlib import md5\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import (Input, Reshape, Dropout, BatchNormalization, Lambda, Dense)\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from multiprocessing import cpu_count\n",
    "from GPUtil import getFirstAvailable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conditional_samples(tf.keras.utils.Sequence):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\"\"\"\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.istraining = config[\"istraining\"]\n",
    "\n",
    "        samples = np.load(os.path.join(config[\"sample_dir\"], \"samples.npz\"))\n",
    "        \n",
    "        self.inputs = samples[\"inputs\"]\n",
    "        self.outputs = samples[\"outputs\"]\n",
    "        \n",
    "        assert len(self.inputs) == len(self.outputs)\n",
    "        \n",
    "        self.inputs = self.inputs[:min(config[\"max_n_samples\"], len(self.inputs))]\n",
    "        self.outputs = self.outputs[:min(config[\"max_n_samples\"], len(self.outputs))]\n",
    "        \n",
    "        self.n_samples = len(self.inputs)\n",
    "        \n",
    "        assert self.n_samples > 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.istraining:\n",
    "            return self.n_samples // self.batch_size # drop remainder\n",
    "        else:\n",
    "             return np.ceil(self.n_samples / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_batch = np.vstack(self.inputs[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        y_batch = np.vstack(self.outputs[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        \n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    sample_path = os.path.join(config[\"sample_dir\"], \"samples.npz\")\n",
    "    \n",
    "    hasher = md5()\n",
    "    hasher.update(open(sample_path, \"rb\").read())\n",
    "    hasher.update(str(config[\"max_n_samples\"]).encode())\n",
    "    hasher.update(str(config[\"batch_size\"]).encode())\n",
    "    hasher.update(str(config[\"epochs\"]).encode())\n",
    "    \n",
    "    save_dir = os.path.join(\"..\", \"models\", config[\"model_name\"], hasher.hexdigest())\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"This model config has been trained already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    # Define Model Function and Loss\n",
    "    \n",
    "    input_shape = np.load(sample_path)[\"inputs\"].shape[1:]\n",
    "    inputs = Input(shape=input_shape, name=\"inputs\")\n",
    "\n",
    "    def model_fn(inputs):\n",
    "\n",
    "        x = Dense(1024, activation=\"relu\")(inputs)\n",
    "\n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "        mu = Dense(512, activation=\"relu\")(x)\n",
    "        mu = Dense(3, activation=\"linear\")(mu)\n",
    "        mu = Lambda(lambda t: K.l2_normalize(t, axis=-1), name=\"mu\")(mu)\n",
    "\n",
    "        kappa = Dense(512, activation=\"relu\")(x)\n",
    "        kappa = Dense(1, activation=\"relu\")(kappa)\n",
    "        kappa = Lambda(lambda t: K.squeeze(t, 1), name=\"kappa\")(kappa) \n",
    "        \n",
    "        return tfp.layers.DistributionLambda(\n",
    "            make_distribution_fn=lambda params: tfd.VonMisesFisher(mean_direction=params[0],\n",
    "                                                                   concentration=params[1]),\n",
    "            convert_to_tensor_fn=tfd.Distribution.mean\n",
    "        )([mu, kappa])\n",
    "\n",
    "    model = tf.keras.Model(inputs, model_fn(inputs), name=config[\"model_name\"])\n",
    "    model.summary()\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    # Run Training\n",
    "    \n",
    "    train_seq = conditional_samples(config)\n",
    "    try:\n",
    "        no_exception = True\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss=negative_log_likelihood\n",
    "        )\n",
    "        train_history = model.fit_generator(\n",
    "            train_seq,\n",
    "            epochs=config[\"epochs\"],\n",
    "            #validation_data=eval_sequence,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=save_dir, write_graph=False),\n",
    "                #ModelCheckpoint(\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\", save_freq=\"epoch\"),\n",
    "                # EarlyStopping(min_delta=0.05, patience=10, restore_best_weights=True, verbose=1)\n",
    "            ],\n",
    "            #validation_steps=1,\n",
    "            max_queue_size=2*config[\"batch_size\"],\n",
    "            use_multiprocessing=True,\n",
    "            workers=cpu_count()\n",
    "        )\n",
    "    #except KeyboardInterrupt:\n",
    "    #    os.rename(save_dir, save_dir + \"_stopped\")\n",
    "    #    save_dir = save_dir + \"_stopped\"\n",
    "    except Exception as e:\n",
    "        shutil.rmtree(save_dir)\n",
    "        no_exception = False\n",
    "        raise e\n",
    "    finally:\n",
    "        if no_exception:\n",
    "            config_path = os.path.join(save_dir, \"config\" + \".yml\")\n",
    "            print(\"Saving {}\".format(config_path))\n",
    "            with open(config_path, \"w\") as file:\n",
    "                yaml.dump(config, file, default_flow_style=False)           \n",
    "\n",
    "            model_path = os.path.join(save_dir, \"model.h5\")\n",
    "            print(\"Saving {}\".format(model_path))\n",
    "            model.save(model_path)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    istraining=True,\n",
    "    model_name=\"entrack_conditional\",\n",
    "    sample_dir=\"../subjects/992774/samples/fa7c02604b92de5f32cd3b61dbc2f8b7\",\n",
    "    max_n_samples=np.inf,\n",
    "    batch_size = 128,\n",
    "    epochs = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 13:01:35.505799 139703451047680 deprecation.py:323] From /local/home/vwegmayr/miniconda2/envs/thesis/lib/python3.6/site-packages/tensorflow_probability/python/distributions/von_mises_fisher.py:312: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"entrack_conditional\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         19456       inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         1049600     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            1539        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            513         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mu (Lambda)                     (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "kappa (Lambda)                  (None,)              0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "distribution_lambda (Distributi ((None, 3), (None, 3 0           mu[0][0]                         \n",
      "                                                                 kappa[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,120,708\n",
      "Trainable params: 2,120,708\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "37456/37456 [==============================] - 270s 7ms/step - loss: -1.4761\n",
      "Saving ../models/entrack_conditional/8d5593b08d4548286cc8564373e82e11/config.yml\n",
      "Saving ../models/entrack_conditional/8d5593b08d4548286cc8564373e82e11/model.h5\n"
     ]
    }
   ],
   "source": [
    "model = train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
