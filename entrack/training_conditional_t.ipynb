{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import nibabel as nib\n",
    "import nipy as ni\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from hashlib import md5\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import (Input, Reshape, Dropout, BatchNormalization, Lambda, Dense)\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from multiprocessing import cpu_count\n",
    "from GPUtil import getFirstAvailable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conditional_samples(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sample_dir, batch_size=256, istraining=True, max_n_samples=np.inf):\n",
    "        \"\"\"\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.istraining = istraining\n",
    "\n",
    "        samples = np.load(os.path.join(sample_dir, \"samples.npz\"))\n",
    "        \n",
    "        self.inputs = samples[\"inputs\"]\n",
    "        self.outputs = samples[\"outputs\"]\n",
    "        \n",
    "        assert len(self.inputs) == len(self.outputs)\n",
    "        \n",
    "        self.inputs = self.inputs[:min(max_n_samples, len(self.inputs))]\n",
    "        self.outputs = self.outputs[:min(max_n_samples, len(self.outputs))]\n",
    "        \n",
    "        self.n_samples = len(self.inputs)\n",
    "        \n",
    "        assert self.n_samples > 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.istraining:\n",
    "            return self.n_samples // self.batch_size # drop remainder\n",
    "        else:\n",
    "             return np.ceil(self.n_samples / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_batch = np.vstack(self.inputs[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        y_batch = np.vstack(self.outputs[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        \n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    hasher = md5()\n",
    "    for v in config.values():\n",
    "        hasher.update(str(v).encode())\n",
    "    \n",
    "    save_dir = os.path.join(\"..\", \"models\", config[\"model_name\"], hasher.hexdigest())\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        print(\"This model config has been trained already:\\n{}\".format(save_dir))\n",
    "        return\n",
    "    \n",
    "    # Define Model Function and Loss\n",
    "    \n",
    "    train_path = os.path.join(config[\"train_dir\"], \"samples.npz\")\n",
    "    \n",
    "    input_shape = np.load(train_path)[\"inputs\"].shape[1:]\n",
    "    inputs = Input(shape=input_shape, name=\"inputs\")\n",
    "\n",
    "    def model_fn(inputs):\n",
    "\n",
    "        x = Dense(2048, activation=\"relu\")(inputs)\n",
    "        x = Dense(2048, activation=\"relu\")(x)\n",
    "        x = Dense(2048, activation=\"relu\")(x)\n",
    "        \n",
    "        # x = Dropout(0.1)(x)\n",
    "\n",
    "        mu = Dense(1024, activation=\"relu\")(x)\n",
    "        mu = Dense(3, activation=\"linear\")(mu)\n",
    "        mu = Lambda(lambda t: K.l2_normalize(t, axis=-1), name=\"mu\")(mu)\n",
    "\n",
    "        kappa = Dense(1024, activation=\"relu\")(x)\n",
    "        kappa = Dense(1, activation=\"relu\")(kappa)\n",
    "        kappa = Lambda(lambda t: K.squeeze(t, 1), name=\"kappa\")(kappa) \n",
    "        \n",
    "        return tfp.layers.DistributionLambda(\n",
    "            make_distribution_fn=lambda params: tfd.VonMisesFisher(mean_direction=params[0],\n",
    "                                                                   concentration=params[1]),\n",
    "            convert_to_tensor_fn=tfd.Distribution.mean\n",
    "        )([mu, kappa])\n",
    "\n",
    "    model = tf.keras.Model(inputs, model_fn(inputs), name=config[\"model_name\"])\n",
    "    model.summary()\n",
    "    \n",
    "    def negative_log_likelihood(observed_y, predicted_distribution):\n",
    "        return -K.mean(predicted_distribution.log_prob(observed_y))\n",
    "    \n",
    "    # Run Training\n",
    "\n",
    "    train_seq = conditional_samples(config[\"train_dir\"], config[\"batch_size\"],\n",
    "                                    max_n_samples=config[\"max_n_samples\"])\n",
    "    #eval_seq = conditional_samples(config[\"eval_dir\"], istraining=False)\n",
    "    try:\n",
    "        no_exception = True\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss=negative_log_likelihood\n",
    "        )\n",
    "        train_history = model.fit_generator(\n",
    "            train_seq,\n",
    "            epochs=config[\"epochs\"],\n",
    "            #validation_data=eval_seq,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=save_dir,\n",
    "                            write_graph=False,\n",
    "                            update_freq=5*config[\"batch_size\"],\n",
    "                            profile_batch=0),\n",
    "                #ModelCheckpoint(os.path.join(save_dir, \"weights.{epoch:02d}-{val_loss:.2f}.h5\"),\n",
    "                #                save_best_only=True),\n",
    "                # EarlyStopping(min_delta=0.05, patience=10, restore_best_weights=True, verbose=1)\n",
    "            ],\n",
    "            max_queue_size=2*config[\"batch_size\"],\n",
    "            use_multiprocessing=True,\n",
    "            workers=cpu_count()\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        os.rename(save_dir, save_dir + \"_stopped\")\n",
    "        save_dir = save_dir + \"_stopped\"\n",
    "    except Exception as e:\n",
    "        shutil.rmtree(save_dir)\n",
    "        no_exception = False\n",
    "        raise e\n",
    "    finally:\n",
    "        if no_exception:\n",
    "            config_path = os.path.join(save_dir, \"config\" + \".yml\")\n",
    "            print(\"Saving {}\".format(config_path))\n",
    "            with open(config_path, \"w\") as file:\n",
    "                yaml.dump(config, file, default_flow_style=False)           \n",
    "\n",
    "            model_path = os.path.join(save_dir, \"model.h5\")\n",
    "            print(\"Saving {}\".format(model_path))\n",
    "            model.save(model_path)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    model_name=\"entrack_conditional\",\n",
    "    train_dir=\"../subjects/992774/samples/7265ad1c1c59d34170b766620efb402c\",\n",
    "    #eval_dir=\"../subjects/992774/samples/0c7f800138a28ef0840333930880d857\",\n",
    "    max_n_samples=np.inf,\n",
    "    batch_size = 256,\n",
    "    epochs = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"entrack_conditional\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 408)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2048)         837632      inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 2048)         4196352     dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2048)         4196352     dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1024)         2098176     dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1024)         2098176     dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 3)            3075        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            1025        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mu (Lambda)                     (None, 3)            0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "kappa (Lambda)                  (None,)              0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "distribution_lambda_2 (Distribu ((None, 3), (None, 3 0           mu[0][0]                         \n",
      "                                                                 kappa[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 13,430,788\n",
      "Trainable params: 13,430,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -1.1698\n",
      "Epoch 2/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -1.4388\n",
      "Epoch 3/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -1.5383\n",
      "Epoch 4/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -1.5990\n",
      "Epoch 5/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -1.6505\n",
      "Epoch 6/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -1.6921\n",
      "Epoch 7/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -1.7296\n",
      "Epoch 8/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -1.7664\n",
      "Epoch 9/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -1.7942\n",
      "Epoch 10/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -1.8257\n",
      "Epoch 11/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -1.8496\n",
      "Epoch 12/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -1.8765\n",
      "Epoch 13/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -1.9041\n",
      "Epoch 14/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -1.9285\n",
      "Epoch 15/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -1.9502\n",
      "Epoch 16/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -1.9782\n",
      "Epoch 17/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.0025\n",
      "Epoch 18/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.0265\n",
      "Epoch 19/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.0482\n",
      "Epoch 20/50\n",
      "1953/1953 [==============================] - 26s 14ms/step - loss: -2.0686\n",
      "Epoch 21/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.1008\n",
      "Epoch 22/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.1206\n",
      "Epoch 23/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.1510\n",
      "Epoch 24/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.1761\n",
      "Epoch 25/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.1987\n",
      "Epoch 26/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.2239\n",
      "Epoch 27/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.2512\n",
      "Epoch 28/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.2789\n",
      "Epoch 29/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.3047\n",
      "Epoch 30/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.3308\n",
      "Epoch 31/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.3578\n",
      "Epoch 32/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.3881\n",
      "Epoch 33/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.4161\n",
      "Epoch 34/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -2.4443\n",
      "Epoch 35/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.4764\n",
      "Epoch 36/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.5015\n",
      "Epoch 37/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.5305\n",
      "Epoch 38/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.5637\n",
      "Epoch 39/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.5935\n",
      "Epoch 40/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.6233\n",
      "Epoch 41/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.6509\n",
      "Epoch 42/50\n",
      "1953/1953 [==============================] - 27s 14ms/step - loss: -2.6867\n",
      "Epoch 43/50\n",
      "1953/1953 [==============================] - 28s 14ms/step - loss: -2.7171\n",
      "Epoch 44/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -2.7451\n",
      "Epoch 45/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.7761\n",
      "Epoch 46/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.8048\n",
      "Epoch 47/50\n",
      "1953/1953 [==============================] - 30s 15ms/step - loss: -2.8369\n",
      "Epoch 48/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.8692\n",
      "Epoch 49/50\n",
      "1953/1953 [==============================] - 29s 15ms/step - loss: -2.8970\n",
      "Epoch 50/50\n",
      "1953/1953 [==============================] - 28s 15ms/step - loss: -2.9267\n",
      "Saving ../models/entrack_conditional/f57290355caf38c41d6aeca1c7dc2091/config.yml\n",
      "Saving ../models/entrack_conditional/f57290355caf38c41d6aeca1c7dc2091/model.h5\n"
     ]
    }
   ],
   "source": [
    "model = train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
